Case 1_ Results:

(1) Deterministic:

P1 >>>>>> 
  VisibleDeprecationWarning)
[[[ 0.  0.  1.  0.]
  [ 1.  0.  0.  0.]
  [ 0.  0.  0.  1.]
  [ 0.  0.  0.  1.]]

 [[ 0.  1.  0.  0.]
  [ 0.  1.  0.  0.]
  [ 0.  1.  0.  0.]
  [ 0.  1.  0.  0.]]]
R1 >>>>>> 
[[   0.  100.]
 [   0.   30.]
 [  50.    0.]
 [  30.  -50.]]


>>>>>>>>>>>>>>Policy and Value Iteration >>>>>>>>>>>>>>>>>>

Iterations with GAMMA >>>>>>>>>> 0.1
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    0.000000
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.2
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    0.000000
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.3
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    0.000000
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.4
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    0.000000
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.5
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           1
         2           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    5.000000
         3    2.500000
         4    1.250000
         5    0.625000
         6    0.312500
         7    0.156250
         8    0.078125
         9    0.039062
        10    0.019531
        11    0.009766
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.6
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           1
         2           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2   12.000000
         3    7.200000
         4    4.320000
         5    2.592000
         6    1.555200
         7    0.933120
         8    0.559872
         9    0.335923
        10    0.201554
        11    0.120932
        12    0.072559
        13    0.043536
        14    0.026121
        15    0.015673
        16    0.009404
        17    0.005642
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.7
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           1
         2           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2   19.000000
         3   13.300000
         4    9.310000
         5    6.517000
         6    4.561900
         7    3.193330
         8    2.235331
         9    1.564732
        10    1.095312
        11    0.766719
        12    0.536703
        13    0.375692
        14    0.262984
        15    0.184089
        16    0.128862
        17    0.090204
        18    0.063143
        19    0.044200
        20    0.030940
        21    0.021658
        22    0.015161
        23    0.010612
        24    0.007429
        25    0.005200
        26    0.003640
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.8
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           1
         2           1
         3           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2   26.000000
         3   20.800000
         4   16.640000
         5   13.312000
         6   10.649600
         7    8.519680
         8    6.815744
         9    5.452595
        10    4.362076
        11    3.489661
        12    2.791729
        13    2.233383
        14    1.786706
        15    1.429365
        16    1.143492
        17    0.914794
        18    0.731835
        19    0.585468
        20    0.468374
        21    0.374699
        22    0.299760
        23    0.239808
        24    0.191846
        25    0.153477
        26    0.122782
        27    0.098225
        28    0.078580
        29    0.062864
        30    0.050291
        31    0.040233
        32    0.032186
        33    0.025749
        34    0.020599
        35    0.016479
        36    0.013184
        37    0.010547
        38    0.008437
        39    0.006750
        40    0.005400
        41    0.004320
        42    0.003456
        43    0.002765
        44    0.002212
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.9
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           1
         2           2
         3           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2   33.000000
         3   29.700000
         4   26.730000
         5   24.057000
         6   21.651300
         7   19.486170
         8   17.537553
         9   15.783798
        10   14.205418
        11   12.784876
        12   11.506389
        13   10.355750
        14    9.320175
        15    8.388157
        16    7.549342
        17    6.794407
        18    6.114967
        19    5.503470
        20    4.953123
        21    4.457811
        22    4.012030
        23    3.610827
        24    3.249744
        25    2.924770
        26    2.632293
        27    2.369063
        28    2.132157
        29    1.918941
        30    1.727047
        31    1.554342
        32    1.398908
        33    1.259017
        34    1.133116
        35    1.019804
        36    0.917824
        37    0.826041
        38    0.743437
        39    0.669093
        40    0.602184
        41    0.541966
        42    0.487769
        43    0.438992
        44    0.395093
        45    0.355584
        46    0.320025
        47    0.288023
        48    0.259221
        49    0.233298
        50    0.209969
        51    0.188972
        52    0.170075
        53    0.153067
        54    0.137760
        55    0.123984
        56    0.111586
        57    0.100427
        58    0.090385
        59    0.081346
        60    0.073212
        61    0.065890
        62    0.059301
        63    0.053371
        64    0.048034
        65    0.043231
        66    0.038908
        67    0.035017
        68    0.031515
        69    0.028364
        70    0.025527
        71    0.022975
        72    0.020677
        73    0.018609
        74    0.016748
        75    0.015074
        76    0.013566
        77    0.012210
        78    0.010989
        79    0.009890
        80    0.008901
        81    0.008011
        82    0.007210
        83    0.006489
        84    0.005840
        85    0.005256
        86    0.004730
        87    0.004257
        88    0.003832
        89    0.003448
        90    0.003104
        91    0.002793
        92    0.002514
        93    0.002262
        94    0.002036
        95    0.001833
        96    0.001649
        97    0.001484
        98    0.001336
        99    0.001202
       100    0.001082
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Total iteration times for value-iteration with different gamma values: 
[2, 2, 2, 2, 11, 17, 26, 44, 100]
Total iteration times for policy-iteration with different gamma values: 
[1, 1, 1, 1, 2, 2, 2, 3, 3]
([2, 2, 2, 2, 11, 17, 26, 44, 100], [1, 1, 1, 1, 2, 2, 2, 3, 3])



>>>>>>>>> Q_Learning >>>>>>>>>>

REAL Values from policy-iteration: 
(526.3157894736844, 473.68421052631595, 426.31578947368433, 376.31578947368433)
REAL policies from policy-iteration:
(1, 0, 1, 1)
-------------------------------------------------------
Greedy factor >>>>>>>> 0.0
Selected Policy: 
(1, 0, 1, 1)
Value: 
(505.091277750382, 453.175367460164, 401.44220462572713, 351.83742124170624)
Greedy factor >>>>>>>> 0.2
Selected Policy: 
(1, 0, 1, 1)
Value: 
(519.3884545675206, 466.97755762278445, 417.7937657390853, 366.0628512778872)
Greedy factor >>>>>>>> 0.4
Selected Policy: 
(1, 0, 1, 1)
Value: 
(523.5456776221505, 470.98182709996365, 422.4053906463763, 368.19320946159235)
Greedy factor >>>>>>>> 0.6
Selected Policy: 
(1, 0, 1, 1)
Value: 
(525.4078936760214, 472.7920624026887, 424.8108352594255, 361.80156359972733)
Greedy factor >>>>>>>> 0.8
Selected Policy: 
(1, 0, 1, 1)
Value: 
(526.0406416060223, 473.41126233308455, 425.63551036900776, 327.44067972225395)
Greedy factor >>>>>>>> 1.0
Selected Policy: 
(1, 0, 1, 1)
Value: 
(525.9122044063305, 473.2822509607182, 413.76283931019384, 159.76962627574392)
Greedy factor >>>>>>>> 1.2
Selected Policy: 
(0, 0, 0, 0)
Value: 
(135.73865971047482, 38.94659144287076, 267.4464295236398, 299.99999785393004)
Greedy factor >>>>>>>> 1.4
Selected Policy: 
(0, 0, 0, 0)
Value: 
(102.38874530262189, 19.34168455304098, 253.77046044309319, 299.99999857380817)
Greedy factor >>>>>>>> 1.6
Selected Policy: 
(0, 0, 0, 0)
Value: 
(108.37330640291768, 22.119595355158822, 256.9777262410534, 299.9999983997949)
Greedy factor >>>>>>>> 1.8
Selected Policy: 
(0, 0, 0, 0)
Value: 
(105.78877977759801, 17.806388160804744, 253.31428764737504, 299.99999856470595)
Greedy factor >>>>>>>> 2.0
Selected Policy: 
(0, 0, 0, 0)
Value: 
(107.99504953352667, 18.553843044843195, 254.90804220910124, 299.99999873457654)
# # # # # # # #
REAL Values from value-iteration: 
(526.3026951700222, 473.6716288110533, 426.30269517002216, 376.30269517002216)
REAL policies from value-iteration:
(1, 0, 1, 1)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Greedy factor >>>>>>>> 0.0
Q: 
[[ 358.13976738  505.09127775]
 [ 453.17536746  436.65811782]
 [ 359.1627448   401.44220463]
 [ 338.2120459   351.83742124]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(505.091277750382, 453.175367460164, 401.44220462572713, 351.83742124170624)
Greedy factor >>>>>>>> 0.2
Q: 
[[ 374.24613236  519.38845457]
 [ 466.97755762  449.64792874]
 [ 372.26376466  417.79376574]
 [ 347.48798992  366.06285128]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(519.3884545675206, 466.97755762278445, 417.7937657390853, 366.0628512778872)
Greedy factor >>>>>>>> 0.4
Q: 
[[ 378.82823055  523.54567762]
 [ 470.9818271   453.44624746]
 [ 367.56535072  422.40539065]
 [ 320.54114713  368.19320946]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(523.5456776221505, 470.98182709996365, 422.4053906463763, 368.19320946159235)
Greedy factor >>>>>>>> 0.6
Q: 
[[ 381.48534006  525.40789368]
 [ 472.7920624   455.15912302]
 [ 337.80944471  424.81083526]
 [ 219.19204256  361.8015636 ]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(525.4078936760214, 472.7920624026887, 424.8108352594255, 361.80156359972733)
Greedy factor >>>>>>>> 0.8
Q: 
[[ 382.27986132  526.04064161]
 [ 473.41126233  455.81150672]
 [ 226.09849272  425.63551037]
 [  87.55838719  327.44067972]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(526.0406416060223, 473.41126233308455, 425.63551036900776, 327.44067972225395)
Greedy factor >>>>>>>> 1.0
Q: 
[[ 335.37929347  525.91220441]
 [ 473.28225096  447.41105169]
 [  24.67505997  413.76283931]
 [  13.81317735  159.76962628]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(525.9122044063305, 473.2822509607182, 413.76283931019384, 159.76962627574392)
Greedy factor >>>>>>>> 1.2
Q: 
[[ 135.73865971    0.        ]
 [  38.94659144    3.67839872]
 [ 267.44642952    0.        ]
 [ 299.99999785  -33.40397615]]
Selected Policy: 
(0, 0, 0, 0)
Value: 
(135.73865971047482, 38.94659144287076, 267.4464295236398, 299.99999785393004)
Greedy factor >>>>>>>> 1.4
Q: 
[[ 102.3887453     0.        ]
 [  19.34168455    0.        ]
 [ 253.77046044    0.        ]
 [ 299.99999857    0.        ]]
Selected Policy: 
(0, 0, 0, 0)
Value: 
(102.38874530262189, 19.34168455304098, 253.77046044309319, 299.99999857380817)
Greedy factor >>>>>>>> 1.6
Q: 
[[ 108.3733064     0.        ]
 [  22.11959536    0.        ]
 [ 256.97772624    0.        ]
 [ 299.9999984   -15.07556723]]
Selected Policy: 
(0, 0, 0, 0)
Value: 
(108.37330640291768, 22.119595355158822, 256.9777262410534, 299.9999983997949)
Greedy factor >>>>>>>> 1.8
Q: 
[[ 105.78877978    0.        ]
 [  17.80638816    0.        ]
 [ 253.31428765    0.        ]
 [ 299.99999856    0.        ]]
Selected Policy: 
(0, 0, 0, 0)
Value: 
(105.78877977759801, 17.806388160804744, 253.31428764737504, 299.99999856470595)
Greedy factor >>>>>>>> 2.0
Q: 
[[ 107.99504953    0.        ]
 [  18.55384304    0.        ]
 [ 254.90804221    0.        ]
 [ 299.99999873    0.        ]]
Selected Policy: 
(0, 0, 0, 0)
Value: 
(107.99504953352667, 18.553843044843195, 254.90804220910124, 299.99999873457654)
(450.65789473684225, [427.88656776949483, 442.55565730181939, 446.28152620752064, 446.20308873446578, 438.13202350759212, 393.18173023824659, 185.53291963272886, 168.87522221814106, 171.86765659973119, 169.22736353762093, 170.36423338051191], 450.64492858027995, [427.88656776949483, 442.55565730181939, 446.28152620752064, 446.20308873446578, 438.13202350759212, 393.18173023824659, 185.53291963272886, 168.87522221814106, 171.86765659973119, 169.22736353762093, 170.36423338051191])




##############################################

(2) Non_deterministic:

P2 >>>>>> 
[[[ 0.   0.   1.   0. ]
  [ 0.9  0.1  0.   0. ]
  [ 0.   0.   0.1  0.9]
  [ 0.   0.   0.   1. ]]

 [[ 0.   1.   0.   0. ]
  [ 0.   1.   0.   0. ]
  [ 0.   0.9  0.1  0. ]
  [ 0.   0.9  0.   0.1]]]
R2 >>>>>> 
[[   0.  100.]
 [   0.   30.]
 [  50.    0.]
 [  30.  -50.]]


>>>>>>>>>>>>>>>>> Policy and Value Iteration >>>>>>>>>>>>>>>>>

Iterations with GAMMA >>>>>>>>>> 0.1
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    0.200000
         3    0.002000
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.2
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    0.400000
         3    0.008000
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.3
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           0
/Library/Python/2.7/site-packages/numpy/core/fromnumeric.py:2641: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
Iterating stopped, unchanging policy found.
  VisibleDeprecationWarning)
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    0.600000
         3    0.018000
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.4
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    0.800000
         3    0.032000
         4    0.001280
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.5
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           1
         2           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    1.500000
         3    0.750000
         4    0.341250
         5    0.170625
         6    0.078478
         7    0.039239
         8    0.018236
         9    0.009118
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.6
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           1
         2           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2    7.800000
         3    4.680000
         4    2.555280
         5    1.533168
         6    0.846208
         7    0.507725
         8    0.283146
         9    0.169887
        10    0.095666
        11    0.057400
        12    0.032613
        13    0.019568
        14    0.011208
        15    0.006725
        16    0.003879
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.7
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           1
         2           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2   14.100000
         3    9.870000
         4    6.287190
         5    4.401033
         6    2.833927
         7    1.983749
         8    1.290671
         9    0.903469
        10    0.593551
        11    0.415486
        12    0.275409
        13    0.192787
        14    0.128826
        15    0.090178
        16    0.060694
        17    0.042486
        18    0.028775
        19    0.020143
        20    0.013717
        21    0.009602
        22    0.006569
        23    0.004599
        24    0.003159
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.8
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           1
         2           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2   20.400000
         3   16.320000
         4   11.880960
         5    9.504768
         6    6.994674
         7    5.595739
         8    4.160813
         9    3.328650
        10    2.499220
        11    1.999376
        12    1.514639
        13    1.211711
        14    0.925377
        15    0.740301
        16    0.569435
        17    0.455548
        18    0.352616
        19    0.282093
        20    0.219546
        21    0.175636
        22    0.137332
        23    0.109866
        24    0.086245
        25    0.068996
        26    0.054343
        27    0.043475
        28    0.034337
        29    0.027470
        30    0.021746
        31    0.017397
        32    0.013799
        33    0.011039
        34    0.008769
        35    0.007016
        36    0.005581
        37    0.004464
        38    0.003555
        39    0.002844
        40    0.002267
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Iterations with GAMMA >>>>>>>>>> 0.9
Policy-iteration ~~~~~~ 
[1 1 0 0]
 Iteration   Variation
         1           1
         2           2
         3           0
Iterating stopped, unchanging policy found.
Value-iteration ~~~~~~ 
None
 Iteration   Variation
         1   70.000000
         2   26.700000
         3   24.030000
         4   19.680570
         5   17.712513
         6   14.664209
         7   13.197788
         8   11.040135
         9    9.936121
        10    8.392780
        11    7.553502
        12    6.437474
        13    2.629338
        14    2.129764
        15    1.725109
        16    1.397338
        17    1.131844
        18    0.916794
        19    0.742603
        20    0.601508
        21    0.487222
        22    0.394650
        23    0.319666
        24    0.258930
        25    0.209733
        26    0.169884
        27    0.137606
        28    0.111461
        29    0.090283
        30    0.073129
        31    0.059235
        32    0.047980
        33    0.038864
        34    0.031480
        35    0.025499
        36    0.020654
        37    0.016730
        38    0.013551
        39    0.010976
        40    0.008891
        41    0.007202
        42    0.005833
        43    0.004725
        44    0.003827
        45    0.003100
        46    0.002511
        47    0.002034
        48    0.001647
        49    0.001334
        50    0.001081
Iterating stopped, epsilon-optimal policy found.
--------------------------------------------------
Total iteration times for value-iteration with different gamma values: 
[3, 3, 3, 4, 9, 16, 24, 40, 50]
Total iteration times for policy-iteration with different gamma values: 
[1, 1, 1, 1, 2, 2, 2, 2, 3]
([3, 3, 3, 4, 9, 16, 24, 40, 50], [1, 1, 1, 1, 2, 2, 2, 2, 3])


>>>>>>>>> Q_Learning >>>>>>>>>>

REAL Values from policy-iteration: 
(502.7624309392268, 447.51381215469644, 398.33647015967483, 343.3914152146199)
REAL policies from policy-iteration:
(1, 0, 1, 1)
-------------------------------------------------------
Greedy factor >>>>>>>> 0.0
Selected Policy: 
(1, 0, 1, 1)
Value: 
(480.7086541382723, 426.3141108662241, 372.9171292264054, 321.4876634103043)
Greedy factor >>>>>>>> 0.2
Selected Policy: 
(1, 0, 1, 1)
Value: 
(496.9907980628868, 443.06862897544994, 390.8911735649782, 335.1409633798485)
Greedy factor >>>>>>>> 0.4
Selected Policy: 
(1, 0, 1, 1)
Value: 
(498.8207656667995, 444.67641678855983, 392.99520993645956, 333.0096834317094)
Greedy factor >>>>>>>> 0.6
Selected Policy: 
(1, 0, 1, 1)
Value: 
(501.7156959992576, 445.84809161674605, 395.80223248456343, 327.55656519159277)
Greedy factor >>>>>>>> 0.8
Selected Policy: 
(1, 0, 1, 1)
Value: 
(503.96366384574384, 449.4132910078274, 400.5022874072458, 298.8315885223241)
Greedy factor >>>>>>>> 1.0
Selected Policy: 
(1, 0, 1, 1)
Value: 
(501.1382884047447, 443.64689442754633, 384.49838258795495, 146.45136842455852)
Greedy factor >>>>>>>> 1.2
Selected Policy: 
(0, 0, 0, 0)
Value: 
(131.3608799594429, 34.656797534688984, 268.98485819295536, 299.99999786697185)
Greedy factor >>>>>>>> 1.4
Selected Policy: 
(0, 0, 0, 0)
Value: 
(104.36109233623309, 19.798645704261148, 256.0987922314979, 299.99999854084297)
Greedy factor >>>>>>>> 1.6
Selected Policy: 
(0, 0, 0, 0)
Value: 
(110.29129792719834, 22.556807005464353, 259.26850636826435, 299.9999983628071)
Greedy factor >>>>>>>> 1.8
Selected Policy: 
(0, 0, 0, 0)
Value: 
(109.0212486629177, 18.543707432161362, 258.59872272265136, 299.9999985048642)
Greedy factor >>>>>>>> 2.0
Selected Policy: 
(0, 0, 0, 0)
Value: 
(111.18930587179817, 19.280601821484996, 260.1388511757693, 299.9999986818177)
# # # # # # # #
REAL Values from value-iteration: 
(500.4113315156664, 445.1631964562232, 395.9854191086232, 341.04036416356826)
REAL policies from value-iteration:
(1, 0, 1, 1)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Greedy factor >>>>>>>> 0.0
Q: 
[[ 332.38799833  480.70865414]
 [ 426.31411087  412.4238512 ]
 [ 337.50212496  372.91712923]
 [ 309.51832337  321.48766341]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(480.7086541382723, 426.3141108662241, 372.9171292264054, 321.4876634103043)
Greedy factor >>>>>>>> 0.2
Q: 
[[ 350.23886886  496.99079806]
 [ 443.06862898  427.21044108]
 [ 351.06635497  390.89117356]
 [ 320.09596319  335.14096338]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(496.9907980628868, 443.06862897544994, 390.8911735649782, 335.1409633798485)
Greedy factor >>>>>>>> 0.4
Q: 
[[ 353.78980368  498.82076567]
 [ 444.67641679  428.7649006 ]
 [ 346.37338654  392.99520994]
 [ 295.47064485  333.00968343]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(498.8207656667995, 444.67641678855983, 392.99520993645956, 333.0096834317094)
Greedy factor >>>>>>>> 0.6
Q: 
[[ 356.89447529  501.715696  ]
 [ 445.84809162  431.71373646]
 [ 322.75933797  395.80223248]
 [ 200.79022323  327.55656519]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(501.7156959992576, 445.84809161674605, 395.80223248456343, 327.55656519159277)
Greedy factor >>>>>>>> 0.8
Q: 
[[ 359.93183383  503.96366385]
 [ 449.41329101  433.97857161]
 [ 230.48232232  400.50228741]
 [  90.15166209  298.83158852]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(503.96366384574384, 449.4132910078274, 400.5022874072458, 298.8315885223241)
Greedy factor >>>>>>>> 1.0
Q: 
[[ 307.80541156  501.1382884 ]
 [ 443.64689443  425.49641041]
 [  36.17176879  384.49838259]
 [  13.39146279  146.45136842]]
Selected Policy: 
(1, 0, 1, 1)
Value: 
(501.1382884047447, 443.64689442754633, 384.49838258795495, 146.45136842455852)
Greedy factor >>>>>>>> 1.2
Q: 
[[ 131.36087996    0.        ]
 [  34.65679753    3.47097345]
 [ 268.98485819    0.        ]
 [ 299.99999787  -21.66977742]]
Selected Policy: 
(0, 0, 0, 0)
Value: 
(131.3608799594429, 34.656797534688984, 268.98485819295536, 299.99999786697185)
Greedy factor >>>>>>>> 1.4
Q: 
[[ 104.36109234    0.        ]
 [  19.7986457     0.        ]
 [ 256.09879223    0.        ]
 [ 299.99999854    0.        ]]
Selected Policy: 
(0, 0, 0, 0)
Value: 
(104.36109233623309, 19.798645704261148, 256.0987922314979, 299.99999854084297)
Greedy factor >>>>>>>> 1.6
Q: 
[[ 110.29129793    0.        ]
 [  22.55680701    0.        ]
 [ 259.26850637    0.        ]
 [ 299.99999836  -15.07556723]]
Selected Policy: 
(0, 0, 0, 0)
Value: 
(110.29129792719834, 22.556807005464353, 259.26850636826435, 299.9999983628071)
Greedy factor >>>>>>>> 1.8
Q: 
[[ 109.02124866    0.        ]
 [  18.54370743    0.        ]
 [ 258.59872272    0.        ]
 [ 299.9999985     0.        ]]
Selected Policy: 
(0, 0, 0, 0)
Value: 
(109.0212486629177, 18.543707432161362, 258.59872272265136, 299.9999985048642)
Greedy factor >>>>>>>> 2.0
Q: 
[[ 111.18930587    0.        ]
 [  19.28060182    0.        ]
 [ 260.13885118    0.        ]
 [ 299.99999868    0.        ]]
Selected Policy: 
(0, 0, 0, 0)
Value: 
(111.18930587179817, 19.280601821484996, 260.1388511757693, 299.9999986818177)
(423.00103211705448, [400.35688941030156, 416.52289099579082, 417.37551895588206, 417.73064632303999, 413.17770769578533, 368.93373346120109, 183.75063338851479, 170.06463220320876, 173.02915241593354, 171.54091933064865, 172.65218938771756], 420.65007781102025, [400.35688941030156, 416.52289099579082, 417.37551895588206, 417.73064632303999, 413.17770769578533, 368.93373346120109, 183.75063338851479, 170.06463220320876, 173.02915241593354, 171.54091933064865, 172.65218938771756])







